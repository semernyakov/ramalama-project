services:
  # ============================================
  # RamaLama - Python CLI/Orchestrator
  # ============================================
  ramalama:
    build:
      context: ./ramalama
      dockerfile: Dockerfile
      args:
        HTTP_PROXY: ${HTTP_PROXY:-}
        HTTPS_PROXY: ${HTTPS_PROXY:-}
        NO_PROXY: ${NO_PROXY:-}
    
    image: ramalama:latest
    container_name: ramalama
    
    user: "1000:100"
    working_dir: /workspace
    
    env_file:
      - config/.env
    
    environment:
      HTTP_PROXY: ${HTTP_PROXY:-}
      HTTPS_PROXY: ${HTTPS_PROXY:-}
      NO_PROXY: ${NO_PROXY:-localhost,127.0.0.0/8,::1}
      
      RAMALAMA_LOG_LEVEL: ${RAMALAMA_LOG_LEVEL:-INFO}
      LLAMA_CPP_SERVER: http://llama-cpp:8080
    
    volumes:
      - ./models:/workspace/models:rw
      - ./cache:/workspace/cache:rw
      - ./logs:/workspace/logs:rw
      - ./data:/workspace/data:rw
      - ./config:/workspace/config:ro
    
    networks:
      - ramalama-net
    
    depends_on:
      llama-cpp:
        condition: service_healthy
    
    healthcheck:
      # test: ["CMD", "ramalama", "version"]
      test: ["CMD", "bash", "-c", "ramalama --version && touch /workspace/logs/healthcheck.test"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    
    profiles: ["cpu", "cuda"]
    
    restart: unless-stopped
    stdin_open: true
    tty: true

  # ============================================
  # llama.cpp - CPU Inference Server
  # ============================================
  llama-cpp:
    image: ghcr.io/ggml-org/llama.cpp:full
    container_name: llama-cpp
    
    user: "1000:100"
    
    ports:
      - "${LLAMA_PORT:-8080}:8080"
    
    env_file:
      - config/.env
    
    environment:
      LLAMA_ARG_HOST: 0.0.0.0
      LLAMA_ARG_PORT: 8080
      LLAMA_ARG_MODEL: ${LLAMA_MODEL:-/models/model.gguf}
      LLAMA_ARG_CTX_SIZE: ${LLAMA_CTX_SIZE:-2048}
      LLAMA_ARG_N_GPU_LAYERS: 0
      LLAMA_ARG_THREADS: ${LLAMA_THREADS:-4}
      LLAMA_ARG_PARALLEL: ${LLAMA_PARALLEL:-1}
    
    volumes:
      - ./models:/models:ro
    
    networks:
      - ramalama-net
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    profiles: ["cpu"]
    
#    deploy:
#      resources:
#        limits:
#          cpus: "${CPU_LIMIT:-4}"
#          memory: "${MEMORY_LIMIT:-8g}"
    
    restart: unless-stopped

    command: ["--server"]


  # ============================================
  # llama.cpp - CUDA Inference Server
  # ============================================
  llama-cpp-cuda:
    image: ghcr.io/ggml-org/llama.cpp:full-cuda
    container_name: llama-cpp-cuda
    
    user: "1000:100"
    
    ports:
      - "${LLAMA_PORT:-8080}:8080"
    
    env_file:
      - config/.env
    
    environment:
      LLAMA_ARG_HOST: 0.0.0.0
      LLAMA_ARG_PORT: 8080
      LLAMA_ARG_MODEL: ${LLAMA_MODEL:-/models/model.gguf}
      LLAMA_ARG_CTX_SIZE: ${LLAMA_CTX_SIZE:-4096}
      LLAMA_ARG_N_GPU_LAYERS: ${LLAMA_GPU_LAYERS:-99}
      LLAMA_ARG_THREADS: ${LLAMA_THREADS:-8}
      LLAMA_ARG_PARALLEL: ${LLAMA_PARALLEL:-2}
      CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-0}
    
    volumes:
      - ./models:/models:ro
    
    networks:
      - ramalama-net
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    profiles: ["cuda"]
    
    deploy:
      resources:
        limits:
          cpus: "${CPU_LIMIT:-8}"
          memory: "${MEMORY_LIMIT:-16g}"
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    restart: unless-stopped

    command: ["--server"]


# ============================================
# Networks
# ============================================
networks:
  ramalama-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
