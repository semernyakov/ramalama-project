# RamaLama Environment Configuration
# Скопируйте этот файл как .env и настройте под себя

# ============================================
# НАСТРОЙКИ ПРОКСИ
# ============================================

# HTTP/HTTPS прокси (если используется)
HTTP_PROXY=http://127.0.0.1:2080
HTTPS_PROXY=http://127.0.0.1:2080

# Исключения для прокси
NO_PROXY=localhost,127.0.0.0/8,::1,host.docker.internal

# ============================================
# НАСТРОЙКИ RAMALAMA
# ============================================

# Путь к моделям внутри контейнера
RAMALAMA_MODELS_PATH=/workspace/models

# Уровень логирования: DEBUG, INFO, WARNING, ERROR
RAMALAMA_LOG_LEVEL=ERROR

# Директория для данных
RAMALAMA_DATA_PATH=/workspace/data

# ============================================
# НАСТРОЙКИ DOCKER
# ============================================

# Имя образа
IMAGE_NAME=ramalama
IMAGE_TAG=latest

# Имя контейнера
CONTAINER_NAME=ramalama

# ============================================
# НАСТРОЙКИ МОДЕЛЕЙ
# ============================================

# Модель по умолчанию для быстрого запуска
DEFAULT_MODEL=llama3.2:1b

# Порт для serve режима
DEFAULT_SERVE_PORT=8080

# ============================================
# ДОПОЛНИТЕЛЬНЫЕ НАСТРОЙКИ
# ============================================

# Лимит памяти для контейнера (опционально)
# MEMORY_LIMIT=8g

# Количество CPU (опционально)
# CPU_LIMIT=4

# GPU support (опционально, требует nvidia-docker)
# USE_GPU=true
