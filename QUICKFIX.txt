═══════════════════════════════════════════════════════════════
  БЫСТРЫЕ ИСПРАВЛЕНИЯ - RamaLama Docker Project
═══════════════════════════════════════════════════════════════

ОБНАРУЖЕНЫ ПРОБЛЕМЫ:
  ✗ Логи прокси повторяются многократно
  ✗ Команда version не работает (--version → version)
  ✗ RamaLama ищет llama-server на хосте (--nocontainer)
  ✗ Ошибка с проверкой памяти в install.sh

═══════════════════════════════════════════════════════════════

БЫСТРОЕ РЕШЕНИЕ:

# Вариант 1: Автоматическое исправление (РЕКОМЕНДУЕТСЯ)
chmod +x apply-hotfix.sh
./apply-hotfix.sh

# Вариант 2: Ручное исправление
./ramalama.sh rebuild

═══════════════════════════════════════════════════════════════

ПОСЛЕ ИСПРАВЛЕНИЯ ПРОВЕРЬТЕ:

1. Версия (без повторяющихся логов):
   ./ramalama.sh -- version

2. Информация:
   ./ramalama.sh info

3. Запуск модели (БЕЗ ошибки про llama-server):
   ./ramalama.sh pull tinyllama
   ./ramalama.sh run tinyllama

═══════════════════════════════════════════════════════════════

ПРАВИЛЬНЫЙ ВЫВОД (после исправлений):

$ ./ramalama.sh -- version
✓ Директории models и data готовы
Creating ramalama-project_ramalama_run ... done
✓ Proxy configured: http://127.0.0.1:2080
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  RamaLama Docker Environment
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Models path: /workspace/models
Proxy: http://127.0.0.1:2080
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
0.15.0

БЕЗ повторяющихся "INFO:ramalama:Using proxy" !

═══════════════════════════════════════════════════════════════

ДОПОЛНИТЕЛЬНАЯ ИНФОРМАЦИЯ: см. HOTFIX.md

═══════════════════════════════════════════════════════════════
