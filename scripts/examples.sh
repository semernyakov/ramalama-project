#!/bin/bash

# ============================================
# RamaLama Quick Examples (Variant B)
# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ
# ============================================

set -euo pipefail

# Ğ¦Ğ²ĞµÑ‚Ğ°
BLUE='\033[0;34m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
NC='\033[0m'

print_section() {
    echo ""
    echo -e "${BLUE}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo -e "${CYAN}$1${NC}"
    echo -e "${BLUE}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
}

print_example() {
    echo ""
    echo -e "${YELLOW}â–¶ $1${NC}"
    echo -e "${GREEN}$ $2${NC}"
}

main() {
    clear
    
    echo -e "${CYAN}"
    cat << "EOF"
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ RamaLama - Quick Start Examples (Variant B)â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EOF
    echo -e "${NC}"
    
    # ============================================
    # Ğ‘Ğ«Ğ¡Ğ¢Ğ Ğ«Ğ™ Ğ¡Ğ¢ĞĞ Ğ¢
    # ============================================
    
    print_section "ğŸš€ QUICK START"
    
    print_example "Build image (first time)" \
        "make buildx"
    echo "ğŸ“ Takes 5-10 min with uv caching"
    
    print_example "Start container" \
        "make up"
    echo "ğŸ“ Container runs in background"
    
    print_example "Check health" \
        "make health"
    echo "ğŸ“ Verify container is ready"
    
    print_example "Download a model" \
        "make pull MODEL=tinyllama"
    echo "ğŸ“ Small model for testing (~2GB)"
    
    print_example "Run interactive chat" \
        "make run MODEL=tinyllama"
    echo "ğŸ“ Type your questions in interactive mode"
    
    # ============================================
    # Ğ—ĞĞŸĞ£Ğ¡Ğš Ğ¡Ğ•Ğ Ğ’Ğ•Ğ Ğ
    # ============================================
    
    print_section "ğŸŒ SERVER MODE (API)"
    
    print_example "Start server" \
        "make serve MODEL=tinyllama PORT=8080"
    echo "ğŸ“ Accessible at http://localhost:8080"
    
    print_example "Test server health" \
        "curl http://localhost:8080/health"
    echo "ğŸ“ Returns JSON with server status"
    
    print_example "Chat via API" \
        "curl -X POST http://localhost:8080/v1/chat/completions \\\\"
    echo "  -H 'Content-Type: application/json' \\\\"
    echo "  -d '{\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'"
    
    # ============================================
    # Ğ£ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ• ĞœĞĞ”Ğ•Ğ›Ğ¯ĞœĞ˜
    # ============================================
    
    print_section "ğŸ¤– MODEL MANAGEMENT"
    
    print_example "List available models" \
        "make list"
    echo "ğŸ“ Shows all downloaded models"
    
    print_example "Download Llama 3.2 (1B)" \
        "make pull MODEL=llama3.2:1b"
    echo "ğŸ“ Recommended: good quality/size balance"
    
    print_example "Check model storage" \
        "./check-models.sh"
    echo "ğŸ“ Verify Variant B persistence"
    
    print_example "Remove model" \
        "docker-compose exec ramalama ramalama rm tinyllama"
    echo "ğŸ“ Frees up space on host"
    
    # ============================================
    # Ğ›ĞĞ“Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• Ğ˜ ĞĞ¢Ğ›ĞĞ”ĞšĞ
    # ============================================
    
    print_section "ğŸ“‹ LOGGING & MONITORING"
    
    print_example "View logs" \
        "make logs"
    echo "ğŸ“ Real-time container logs"
    
    print_example "Manage log files" \
        "./log-manager.sh status"
    echo "ğŸ“ Check log storage info"
    
    print_example "Monitor system" \
        "./monitor.sh -s"
    echo "ğŸ“ One-time snapshot of system state"
    
    print_example "Interactive monitoring" \
        "./monitor.sh -i"
    echo "ğŸ“ Auto-refresh dashboard (Ctrl+C to exit)"
    
    # ============================================
    # Ğ˜ĞĞ¢Ğ•Ğ ĞĞšĞ¢Ğ˜Ğ’ĞĞĞ¯ Ğ ĞĞ‘ĞĞ¢Ğ
    # ============================================
    
    print_section "ğŸ’» INTERACTIVE SHELL"
    
    print_example "Enter container shell" \
        "make shell"
    echo "ğŸ“ Full bash access inside container"
    
    print_example "Example commands in shell:" \
        ""
    echo "  ramalama list           # List models"
    echo "  ramalama info           # Show config"
    echo "  ramalama run tinyllama  # Chat mode"
    
    # ============================================
    # BATCH ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ
    # ============================================
    
    print_section "âš¡ BATCH PROCESSING"
    
    print_example "Process text file" \
        "cat questions.txt | make run MODEL=tinyllama"
    echo "ğŸ“ Pipe questions into model"
    
    print_example "Save results to file" \
        "echo 'What is AI?' | make run MODEL=tinyllama > response.txt"
    echo "ğŸ“ Redirect output to file"
    
    # ============================================
    # ĞŸĞĞ›Ğ•Ğ—ĞĞ«Ğ• ĞšĞĞœĞĞĞ”Ğ«
    # ============================================
    
    print_section "ğŸ”§ USEFUL COMMANDS"
    
    print_example "All available commands" \
        "make help"
    echo "ğŸ“ Shows all make targets"
    
    print_example "Check config" \
        "make config"
    echo "ğŸ“ Display ./config/.env contents"
    
    print_example "Test setup" \
        "make test"
    echo "ğŸ“ Quick system sanity checks"
    
    print_example "Stop container" \
        "make down"
    echo "ğŸ“ Clean shutdown"
    
    print_example "Rebuild image" \
        "make rebuild"
    echo "ğŸ“ Clean build without cache"
    
    # ============================================
    # Ğ’ĞĞ Ğ˜ĞĞĞ¢Ğ« Ğ‘ ĞĞ¡ĞĞ‘Ğ•ĞĞĞĞ¡Ğ¢Ğ˜
    # ============================================
    
    print_section "âœ¨ VARIANT B FEATURES"
    
    echo ""
    echo -e "${GREEN}âœ“ Models${NC} persisted on host ./models (automatic backups)"
    echo -e "${GREEN}âœ“ Logs${NC}    local in container (no disk clutter on host)"
    echo -e "${GREEN}âœ“ Data${NC}    local in container (ephemeral)"
    echo -e "${GREEN}âœ“ Cache${NC}   local in container (fast performance)"
    echo -e "${GREEN}âœ“ Config${NC}  from ./config/.env (environment variables)"
    echo ""
    echo "ğŸ¯ Perfect for: Development, testing, model experimentation"
    echo ""
    
    # ============================================
    # ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ•Ğ’
    # ============================================
    
    print_section "ğŸ“š PRACTICAL SCENARIOS"
    
    echo ""
    echo -e "${YELLOW}Scenario 1: Document Summarization${NC}"
    echo "$ cat report.pdf | pdftotext - - | make run MODEL=llama3.2:1b"
    echo ""
    
    echo -e "${YELLOW}Scenario 2: Code Generation${NC}"
    echo "$ echo 'Write Python function to reverse a string' | \\\\"
    echo "  make run MODEL=llama3.2:1b"
    echo ""
    
    echo -e "${YELLOW}Scenario 3: API Server for App${NC}"
    echo "$ make serve MODEL=llama3.2:1b PORT=8000 &"
    echo "$ curl http://localhost:8000/v1/chat/completions -d '{...}'"
    echo ""
    
    echo -e "${YELLOW}Scenario 4: Batch Document Processing${NC}"
    echo "$ for file in documents/*.txt; do"
    echo "    make run MODEL=llama3.2:1b < \"\$file\" > results/\$(basename \$file)"
    echo "  done"
    echo ""
    
    # ============================================
    # TIPS & TRICKS
    # ============================================
    
    print_section "ğŸ’¡ TIPS & TRICKS"
    
    echo ""
    echo -e "${GREEN}Tip 1:${NC} Create shell aliases for faster workflow"
    echo "  alias rlm='docker-compose exec ramalama ramalama'"
    echo "  alias rlm-serve='make serve'"
    echo ""
    
    echo -e "${GREEN}Tip 2:${NC} Use smaller models for faster responses"
    echo "  tinyllama      - Fastest (242M, 2GB)"
    echo "  llama3.2:1b    - Balanced (1B, 3GB)"
    echo "  llama3.2:3b    - Quality (3B, 7GB)"
    echo ""
    
    echo -e "${GREEN}Tip 3:${NC} Monitor system while running"
    echo "  ./monitor.sh -s        # One-time snapshot"
    echo "  ./monitor.sh -i        # Interactive mode"
    echo "  REFRESH_INTERVAL=3 ./monitor.sh -i  # Faster updates"
    echo ""
    
    echo -e "${GREEN}Tip 4:${NC} Save frequently used prompts"
    echo "  mkdir prompts/"
    echo "  echo 'Summarize in 3 bullet points' > prompts/summarize.txt"
    echo "  cat doc.txt prompts/summarize.txt | make run"
    echo ""
    
    # ============================================
    # TROUBLESHOOTING
    # ============================================
    
    print_section "ğŸ” TROUBLESHOOTING"
    
    echo ""
    echo -e "${YELLOW}Q: Container won't start?${NC}"
    echo "A: Check logs with: make logs"
    echo "   Or rebuild: make rebuild"
    echo ""
    
    echo -e "${YELLOW}Q: Models not persisting?${NC}"
    echo "A: Check with: ./check-models.sh"
    echo "   Verify mount: docker inspect ramalama"
    echo ""
    
    echo -e "${YELLOW}Q: Slow responses?${NC}"
    echo "A: Check system resources: ./monitor.sh -s"
    echo "   Try smaller model: make serve MODEL=tinyllama"
    echo ""
    
    echo -e "${YELLOW}Q: Out of disk space?${NC}"
    echo "A: Clean old logs: ./log-manager.sh clean"
    echo "   Remove unused models: make list && rm models/unused.gguf"
    echo ""
    
    # ============================================
    # Ğ”ĞĞŸĞĞ›ĞĞ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ
    # ============================================
    
    print_section "ğŸ“– MORE HELP"
    
    echo ""
    echo "Documentation files:"
    echo "  README.md              - Full documentation"
    echo "  config/.env            - Configuration"
    echo "  Makefile               - All available targets"
    echo "  Dockerfile             - Image specification"
    echo "  docker-compose.yml     - Container setup"
    echo ""
    
    echo "Quick help:"
    echo "  make help              - Show all make targets"
    echo "  ./log-manager.sh help  - Log management help"
    echo "  ./monitor.sh --help    - Monitor help"
    echo "  ./setup-dirs.sh        - Check workspace structure"
    echo ""
    
    echo -e "${GREEN}ğŸš€ Happy experimenting with RamaLama!${NC}"
    echo ""
}

main "$@"
