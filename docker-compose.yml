version: '3.8'

services:
  ramalama:
    build:
      context: .
      dockerfile: Dockerfile
    image: ramalama:latest
    container_name: ramalama
    
    ports:
      - "8080:8080"
    
    networks:
      - ramalama-network
    
    # Load environment from config/.env
    env_file:
      - config/.env
    
    # Override/extend environment variables
    environment:
      # CRITICAL: Models directory (in container) - for RamaLama compatibility
      - RAMALAMA_STORE=/workspace/models
      
      # Explicit paths (from config/.env, but can override here)
      - RAMALAMA_MODELS_PATH=/workspace/models
      - RAMALAMA_DATA_PATH=/workspace/data
      - RAMALAMA_LOG_FILE=/workspace/logs/ramalama.log
      - RAMALAMA_ENGINE=llama.cpp
      
      # Proxy from config/.env (${HTTP_PROXY:-} will use .env value or empty)
      - HTTP_PROXY=${HTTP_PROXY:-}
      - HTTPS_PROXY=${HTTPS_PROXY:-}
      - NO_PROXY=${NO_PROXY:-localhost,127.0.0.0/8,::1,host.docker.internal}
      
      # Hugging Face
      - HF_HOME=/workspace/cache
      - HF_HUB_DISABLE_PROGRESS_BARS=${HF_HUB_DISABLE_PROGRESS_BARS:-false}
      - HF_HUB_ENABLE_HF_TRANSFER=${HF_HUB_ENABLE_HF_TRANSFER:-1}
      
      # Log level from config/.env
      - RAMALAMA_LOG_LEVEL=${RAMALAMA_LOG_LEVEL:-ERROR}
    
    volumes:
      # VARIANT B: Only mount models for persistence
      - ./models:/workspace/models:rw
      # Symlink for llama.cpp compatibility (if it expects /mnt/models)
      - /workspace/models:/mnt/models:rw
      # Logs stay inside container (can be extracted via docker cp if needed)
      # Data stays inside container
      # Cache stays inside container
      
      # Optional: mount config as read-only for reference
      - ./config:/workspace/config:ro
    
    # Default waiting mode
    command: tail -f /dev/null
    stdin_open: true
    tty: true
    restart: unless-stopped
    
    # Resource limits (from config/.env or defaults)
    # Uncomment and adjust based on your system
    # deploy:
    #   resources:
    #     limits:
    #       memory: ${MEMORY_LIMIT:-4g}
    #       cpus: ${CPU_LIMIT:-4}

# Network
networks:
  ramalama-network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_ip_masquerade: "true"