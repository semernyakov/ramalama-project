version: '3.8'

services:
  ramalama:
    build: .
    image: ramalama:latest
    container_name: ramalama
    
    # Используем host network для доступа к прокси на 127.0.0.1
    network_mode: host
    
    env_file:
      - config/.env
    
    environment:
      # Прокси настройки (из .env)
      - HTTP_PROXY=${HTTP_PROXY:-}
      - HTTPS_PROXY=${HTTPS_PROXY:-}
      - NO_PROXY=localhost,127.0.0.0/8,::1
      
      # КРИТИЧНО: Указываем RamaLama где хранить модели
      - RAMALAMA_STORE=/var/lib/ramalama
      - RAMALAMA_IN_CONTAINER=1
      - RAMALAMA_LOG_LEVEL=ERROR
      
      # Пути для логов
      - RAMALAMA_LOG_FILE=/workspace/logs/ramalama.log
      
      # Настройки Hugging Face
      - HF_HUB_DISABLE_PROGRESS_BARS=false
      - HF_HUB_ENABLE_HF_TRANSFER=1
      
      # Указываем движок для RamaLama
      
      # Указываем образ для запуска моделей
    
    volumes:
      # КРИТИЧНО: Монтируем /var/lib/ramalama для сохранения моделей!
      - ./models:/var/lib/ramalama:rw
      
      # Логи в отдельную директорию
      - ./logs:/workspace/logs:rw
      
      # Пользовательские данные
      - ./data:/workspace/data:rw
      
      # Конфиги в отдельную директорию
      - ./config:/workspace/config:ro
    
    # По умолчанию контейнер в режиме ожидания
    command: tail -f /dev/null
    
    stdin_open: true
    tty: true
    restart: unless-stopped

  # Альтернативный сервис без прокси
  ramalama-no-proxy:
    build: .
    image: ramalama:latest
    container_name: ramalama-no-proxy
    profiles: ["no-proxy"]
    
    environment:
      - RAMALAMA_STORE=/var/lib/ramalama
      - RAMALAMA_IN_CONTAINER=1
      - RAMALAMA_LOG_LEVEL=ERROR
      - RAMALAMA_LOG_FILE=/workspace/logs/ramalama.log
    
    volumes:
      - ./models:/var/lib/ramalama:rw
      - ./logs:/workspace/logs:rw
      - ./data:/workspace/data:rw
      - ./config:/workspace/config:ro
    
    command: tail -f /dev/null
    
    stdin_open: true
    tty: true
    restart: unless-stopped
